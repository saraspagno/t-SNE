{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10316d5779a3733",
   "metadata": {
    "collapsed": false,
    "id": "10316d5779a3733"
   },
   "source": [
    "# Exercise 1: t-SNE\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "\n",
    "* The homework assignments are executed automatically.\n",
    "* Failure to comply with the following instructions will result in a significant penalty.\n",
    "* Appeals regarding your failure to read these instructions will be denied.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/).\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes a zip file that contains this notebook, with your ID as the file name. For example, `hw1_123456789_987654321.zip` if you submitted in pairs and `hw1_123456789.zip` if you submitted the exercise alone. The name of the notebook should follow the same structure.\n",
    "   \n",
    "Please use only a **zip** file in your submission.\n",
    "\n",
    "---\n",
    "##❗❗❗❗❗❗❗❗❗**This is mandatory**❗❗❗❗❗❗❗❗❗\n",
    "## Please write your RUNI emails in this cell:\n",
    "\n",
    "### *** YOUR EMAILS HERE ***\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions:\n",
    "\n",
    "### *** YOUR IDS HERE ***\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735832cbfa43f83",
   "metadata": {
    "id": "735832cbfa43f83"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c",
   "metadata": {
    "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c"
   },
   "source": [
    "# Design your algorithm\n",
    "Make sure to describe the algorithm, its limitations, and describe use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7760b-b4ef-45a9-9aad-88b45fd8d442",
   "metadata": {
    "id": "0ba7760b-b4ef-45a9-9aad-88b45fd8d442"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2158ff2629daf2bb",
   "metadata": {
    "collapsed": false,
    "id": "2158ff2629daf2bb"
   },
   "source": [
    "# Your implementations\n",
    "You may add new cells, write helper functions or test code as you see fit.\n",
    "Please use the cell below and include a description of your implementation.\n",
    "Explain code design consideration, algorithmic choices and any other details you think is relevant to understanding your implementation.\n",
    "Failing to explain your code will lead to point deductions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe206c9-d1f4-440b-aca0-c807cdd79451",
   "metadata": {
    "id": "afe206c9-d1f4-440b-aca0-c807cdd79451"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b85d8f7447ebce0",
   "metadata": {
    "id": "7b85d8f7447ebce0"
   },
   "outputs": [],
   "source": [
    "class CustomTSNE:\n",
    "    def __init__(\n",
    "        self, perplexity=30.0, n_components=2, n_iter=1000, learning_rate=200.0\n",
    "    ):\n",
    "        self.perplexity = perplexity\n",
    "        self.n_components = n_components\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        # Note: You may add more attributes\n",
    "\n",
    "    def _pairwise_distances(self, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Compute pairwise squared Euclidean distances between all samples in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input data matrix where each row represents one data point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        D : ndarray of shape (n_samples, n_samples)\n",
    "            Matrix of squared Euclidean distances.\n",
    "        \"\"\"\n",
    "        # Ensure floating-point type for numerical stability\n",
    "        X = X.astype(np.float64)\n",
    "\n",
    "        # ||x_i - x_j||^2 = (x_i·x_i) + (x_j·x_j) − 2*(x_i·x_j)\n",
    "        sum_X = np.sum(np.square(X), axis=1)\n",
    "        D = np.add.outer(sum_X, sum_X) - 2 * np.dot(X, X.T)\n",
    "\n",
    "        # Set diagonal to 0 to avoid small numerical residuals\n",
    "        np.fill_diagonal(D, 0.0)\n",
    "\n",
    "        return D\n",
    "\n",
    "    def _binary_search_sigma(\n",
    "        self,\n",
    "        distances_i: np.ndarray,\n",
    "        target_perplexity: float,\n",
    "        tol: float = 1e-5,\n",
    "        max_iter: int = 50,\n",
    "    ) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Find the Gaussian width σ_i for one data point so that its conditional\n",
    "        probability distribution p_{j|i} has the desired perplexity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        distances_i : ndarray of shape (n_samples,)\n",
    "            Squared distances from the i-th point to all other points.\n",
    "        target_perplexity : float\n",
    "            Desired perplexity (effective number of neighbors).\n",
    "        tol : float, default=1e-5\n",
    "            Stopping tolerance for perplexity difference.\n",
    "        max_iter : int, default=50\n",
    "            Maximum number of binary-search iterations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p_i : ndarray of shape (n_samples,)\n",
    "            Conditional probabilities p_{j|i} (sum to 1, p_{i|i}=0).\n",
    "        sigma : float\n",
    "            Optimal σ_i value that achieved the target perplexity.\n",
    "        \"\"\"\n",
    "        # β = 1 / (2σ²)\n",
    "        beta_min, beta_max = -np.inf, np.inf\n",
    "        beta = 1.0\n",
    "\n",
    "        # Ensure float type and ignore self-distance\n",
    "        distances_i = distances_i.astype(np.float64).copy()\n",
    "        distances_i[distances_i == 0] = np.inf\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            # Compute Gaussian affinities\n",
    "            P_i = np.exp(-distances_i * beta)\n",
    "            P_i[distances_i == np.inf] = 0.0\n",
    "            sum_P_i = np.sum(P_i)\n",
    "            if sum_P_i == 0:\n",
    "                sum_P_i = 1e-12\n",
    "            P_i /= sum_P_i\n",
    "\n",
    "            # Compute Shannon entropy and perplexity\n",
    "            H = -np.sum(P_i * np.log2(P_i + 1e-10))\n",
    "            perplexity = 2**H\n",
    "\n",
    "            # Check for convergence\n",
    "            if abs(perplexity - target_perplexity) < tol:\n",
    "                break\n",
    "\n",
    "            # Adjust β (inverse variance) based on whether entropy too high/low\n",
    "            if perplexity > target_perplexity:\n",
    "                beta_min = beta\n",
    "                beta = beta * 2 if beta_max == np.inf else (beta + beta_max) / 2\n",
    "            else:\n",
    "                beta_max = beta\n",
    "                beta = beta / 2 if beta_min == -np.inf else (beta + beta_min) / 2\n",
    "\n",
    "        sigma = np.sqrt(1 / (2 * beta))\n",
    "        return P_i, sigma\n",
    "\n",
    "    def _compute_P(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the symmetric joint probability matrix P from high-dimensional data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input data matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        P : ndarray of shape (n_samples, n_samples)\n",
    "            Symmetric joint probability matrix where P[i, j] reflects similarity\n",
    "            between points i and j in high-dimensional space.\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # Edge case: only one point\n",
    "        if N == 1:\n",
    "            return np.zeros((1, 1))\n",
    "\n",
    "        # Compute squared distances (ensure float type)\n",
    "        distances = self._pairwise_distances(X).astype(np.float64)\n",
    "        P = np.zeros((N, N))\n",
    "        sigmas = np.zeros(N)\n",
    "\n",
    "        # Compute conditional probabilities for each data point\n",
    "        for i in range(N):\n",
    "            # Handle identical points (all distances ≈ 0)\n",
    "            if np.allclose(distances[i], 0):\n",
    "                P[i] = np.zeros(N)\n",
    "                continue\n",
    "            p_i, sigma_i = self._binary_search_sigma(distances[i], self.perplexity)\n",
    "            P[i] = p_i\n",
    "            sigmas[i] = sigma_i\n",
    "\n",
    "        # Symmetrize and normalize\n",
    "        P = (P + P.T) / (2 * N)\n",
    "        np.fill_diagonal(P, 0.0)\n",
    "\n",
    "        # Prevent underflow and re-normalize\n",
    "        P = np.maximum(P, 1e-12)\n",
    "        P /= np.sum(P)\n",
    "\n",
    "        return P\n",
    "\n",
    "    def _compute_Q(self, Y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute low-dimensional joint probability matrix Q using Student-t distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : ndarray of shape (n_samples, n_components)\n",
    "            Current low-dimensional embedding.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Q : ndarray of shape (n_samples, n_samples)\n",
    "            Symmetric joint probability matrix in low-dimensional space.\n",
    "        num : ndarray of shape (n_samples, n_samples)\n",
    "            Unnormalized pairwise affinities (for gradient reuse).\n",
    "        \"\"\"\n",
    "        # Edge case: single point\n",
    "        if Y.shape[0] == 1:\n",
    "            return np.zeros((1, 1)), np.zeros((1, 1))\n",
    "\n",
    "        # Compute pairwise squared distances\n",
    "        sum_Y = np.sum(np.square(Y), axis=1)\n",
    "        D = np.add.outer(sum_Y, sum_Y) - 2 * np.dot(Y, Y.T)\n",
    "\n",
    "        # Student-t affinities\n",
    "        num = 1 / (1 + D)\n",
    "        np.fill_diagonal(num, 0.0)\n",
    "\n",
    "        # Normalize\n",
    "        Q = num / np.sum(num)\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    "\n",
    "        return Q, num\n",
    "    \n",
    "    def _gradient(self, P: np.ndarray, Q: np.ndarray, Y: np.ndarray, num: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the KL divergence loss with respect to Y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        P : ndarray of shape (n_samples, n_samples)\n",
    "            High-dimensional joint probabilities.\n",
    "        Q : ndarray of shape (n_samples, n_samples)\n",
    "            Low-dimensional joint probabilities.\n",
    "        Y : ndarray of shape (n_samples, n_components)\n",
    "            Current low-dimensional embedding.\n",
    "        num : ndarray of shape (n_samples, n_samples)\n",
    "            Unnormalized affinities (1 / (1 + ||y_i - y_j||^2)), reused for efficiency.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dY : ndarray of shape (n_samples, n_components)\n",
    "            Gradient of the KL divergence with respect to Y.\n",
    "        \"\"\"\n",
    "        PQ = P - Q  # difference between high-D and low-D similarities\n",
    "        # (P - Q) * num gives the force magnitude per pair\n",
    "        forces = 4 * (PQ * num)  # scalar weight for each pair\n",
    "\n",
    "        # Compute gradient as matrix operation\n",
    "        dY = np.zeros_like(Y)\n",
    "        for i in range(Y.shape[0]):\n",
    "            # weighted sum of differences (y_i - y_j)\n",
    "            dY[i, :] = np.sum(np.expand_dims(forces[i, :], 1) * (Y[i, :] - Y), axis=0)\n",
    "\n",
    "        return dY\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit t-SNE on high-dimensional data X and return low-dimensional embedding Y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input high-dimensional data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Y : ndarray of shape (n_samples, n_components)\n",
    "            Low-dimensional t-SNE embedding.\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # Edge case: all points identical → no embedding needed\n",
    "        if np.allclose(X, X[0]):\n",
    "            return np.zeros((N, self.n_components))\n",
    "\n",
    "        # Step 1: Compute high-dimensional similarities P\n",
    "        P = self._compute_P(X)\n",
    "\n",
    "        # Step 2: Initialize Y deterministically for reproducibility\n",
    "        rng = np.random.default_rng(42)\n",
    "        Y = rng.normal(0, 1e-4, size=(N, self.n_components))\n",
    "\n",
    "        # Step 3: Early exaggeration (standard = ×4)\n",
    "        early_exaggeration = 4.0\n",
    "        P = np.maximum(P * early_exaggeration, 1e-12)\n",
    "\n",
    "        # Step 4: Learning-rate heuristics (like sklearn’s “auto”)\n",
    "        if isinstance(self.learning_rate, str) and self.learning_rate == \"auto\":\n",
    "            self.learning_rate = max(N / early_exaggeration / 4, 50)\n",
    "\n",
    "        # Momentum configuration\n",
    "        momentum = 0.5\n",
    "        final_momentum = 0.8\n",
    "        momentum_switch_iter = 250\n",
    "        dY_prev = np.zeros_like(Y)\n",
    "\n",
    "        prev_kl = np.inf\n",
    "\n",
    "        # Step 5: Gradient-descent loop\n",
    "        for it in range(self.n_iter):\n",
    "            Q, num = self._compute_Q(Y)\n",
    "            grad = self._gradient(P, Q, Y, num)\n",
    "\n",
    "            # Gradient update with momentum\n",
    "            dY = momentum * dY_prev - self.learning_rate * grad\n",
    "            Y += dY\n",
    "            dY_prev = dY\n",
    "\n",
    "            # Clip to prevent runaway values\n",
    "            Y = np.clip(Y, -50, 50)\n",
    "\n",
    "            # Compute KL divergence for monitoring\n",
    "            kl = np.sum(P * np.log((P + 1e-12) / (Q + 1e-12)))\n",
    "\n",
    "            # Reduce learning rate if divergence explodes\n",
    "            if kl > prev_kl * 4:\n",
    "                self.learning_rate *= 0.5\n",
    "            prev_kl = kl\n",
    "\n",
    "            # Switch momentum after warm-up phase\n",
    "            if it == momentum_switch_iter:\n",
    "                momentum = final_momentum\n",
    "\n",
    "            # Reduce early exaggeration after 100 iters\n",
    "            if it == 100:\n",
    "                P /= early_exaggeration\n",
    "\n",
    "            # Optional progress logging\n",
    "            if (it + 1) % 100 == 0:\n",
    "                print(f\"Iteration {it + 1}: KL divergence = {kl:.5f}\")\n",
    "\n",
    "        return Y\n",
    "\n",
    "    # Part 2: Transformation of New Data Points\n",
    "    def transform(self, X_original, Y_original, X_new):\n",
    "        # Implement your method for incorporating new points into the existing t-SNE layout\n",
    "        # Your code here\n",
    "\n",
    "        # Return Y_new, the transformed data\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca742614",
   "metadata": {},
   "source": [
    "# Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b72886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".................."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: KL divergence = 11.00003\n",
      "Iteration 200: KL divergence = 0.36281\n",
      "Iteration 300: KL divergence = 0.12922\n",
      "Iteration 100: KL divergence = 20.09445\n",
      "Iteration 200: KL divergence = 0.57472\n",
      "Iteration 300: KL divergence = 0.31651\n",
      "Iteration 100: KL divergence = 10.60991\n",
      "Iteration 200: KL divergence = 0.20323\n",
      "Iteration 100: KL divergence = 10.60991\n",
      "Iteration 200: KL divergence = 0.20323\n",
      "Iteration 100: KL divergence = 8.48824\n",
      "Iteration 200: KL divergence = -0.00000\n",
      "Iteration 300: KL divergence = -0.00000\n",
      "Iteration 400: KL divergence = -0.00000\n",
      "Iteration 500: KL divergence = -0.00000\n",
      "Iteration 600: KL divergence = -0.00000\n",
      "Iteration 700: KL divergence = -0.00000\n",
      "Iteration 800: KL divergence = -0.00000\n",
      "Iteration 900: KL divergence = -0.00000\n",
      "Iteration 1000: KL divergence = -0.00000\n",
      "Iteration 100: KL divergence = 12.79132\n",
      "Iteration 200: KL divergence = 1.04135\n",
      "Iteration 300: KL divergence = 0.67337\n",
      "Iteration 100: KL divergence = 8.98821\n",
      "Iteration 200: KL divergence = 0.42365\n",
      "Iteration 300: KL divergence = 0.40866\n",
      "Iteration 100: KL divergence = 8.48824\n",
      "Iteration 200: KL divergence = -0.00000\n",
      "Iteration 300: KL divergence = -0.00000\n",
      "Iteration 400: KL divergence = -0.00000\n",
      "Iteration 500: KL divergence = -0.00000\n",
      "Iteration 600: KL divergence = -0.00000\n",
      "Iteration 700: KL divergence = -0.00000\n",
      "Iteration 800: KL divergence = -0.00000\n",
      "Iteration 900: KL divergence = -0.00000\n",
      "Iteration 1000: KL divergence = -0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 24 tests in 0.218s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=24 errors=0 failures=0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "class TestCustomTSNE(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        \"\"\"Small sample matrix for testing.\"\"\"\n",
    "        self.tsne = CustomTSNE()\n",
    "        self.X = np.array([[0, 0], [3, 4]])\n",
    "    \n",
    "    def test_pairwise_distances(self):\n",
    "        \"\"\"Check if pairwise distance matrix is correct.\"\"\"\n",
    "        D = self.tsne._pairwise_distances(self.X)\n",
    "        expected = np.array([[0., 25.], [25., 0.]])\n",
    "        self.assertTrue(np.allclose(D, expected), \"Pairwise distance computation failed\")\n",
    "    \n",
    "    def test_binary_search_sigma(self):\n",
    "        \"\"\"Check if computed perplexity is close to target.\"\"\"\n",
    "        D = np.array([0., 1., 4., 9.])  # distances from one point to 3 others\n",
    "        p_i, sigma = self.tsne._binary_search_sigma(D, target_perplexity=2.0)\n",
    "        \n",
    "        # Compute achieved perplexity\n",
    "        H = -np.sum(p_i * np.log2(p_i + 1e-10))\n",
    "        perplexity = 2 ** H\n",
    "        \n",
    "        self.assertAlmostEqual(perplexity, 2.0, delta=0.1, msg=\"Perplexity not matched\")\n",
    "        self.assertAlmostEqual(np.sum(p_i), 1.0, delta=1e-6, msg=\"Probabilities not normalized\")\n",
    "    \n",
    "    def test_compute_P(self):\n",
    "        \"\"\"Check if the P matrix is valid and normalized.\"\"\"\n",
    "        P = self.tsne._compute_P(self.X)\n",
    "        \n",
    "        # P must be symmetric\n",
    "        self.assertTrue(np.allclose(P, P.T, atol=1e-8), \"P is not symmetric\")\n",
    "        # Sum of all probabilities should be 1\n",
    "        self.assertAlmostEqual(np.sum(P), 1.0, delta=1e-6, msg=\"P not normalized\")\n",
    "        # Diagonal should be 0\n",
    "        self.assertTrue(np.allclose(np.diag(P), 0.0), \"Diagonal of P should be 0\")\n",
    "        # All probabilities should be positive\n",
    "        self.assertTrue(np.all(P >= 0), \"P contains negative values\")\n",
    "    \n",
    "    def test_compute_P_single_point(self):\n",
    "        \"\"\"Edge case: single data point should yield all zeros.\"\"\"\n",
    "        X_single = np.array([[0.0, 0.0]])\n",
    "        P = self.tsne._compute_P(X_single)\n",
    "        self.assertTrue(np.allclose(P, np.zeros_like(P)), \"Single point should return zeros\")\n",
    "\n",
    "    def test_compute_P_identical_points(self):\n",
    "        \"\"\"Edge case: identical points should not produce NaN or inf.\"\"\"\n",
    "        X_identical = np.array([[1.0, 1.0], [1.0, 1.0]])\n",
    "        P = self.tsne._compute_P(X_identical)\n",
    "        self.assertFalse(np.any(np.isnan(P)), \"P contains NaNs\")\n",
    "        self.assertFalse(np.any(np.isinf(P)), \"P contains infinities\")\n",
    "        self.assertAlmostEqual(np.sum(P), 1.0, delta=1e-6, msg=\"P not normalized\")\n",
    "\n",
    "    def test_compute_P_large_values(self):\n",
    "        \"\"\"Edge case: large feature magnitudes should not overflow.\"\"\"\n",
    "        X_large = np.array([[1000, 1000], [1003, 1004], [1006, 1008]])\n",
    "        P = self.tsne._compute_P(X_large)\n",
    "        self.assertTrue(np.all(P >= 0), \"P contains negative values\")\n",
    "        self.assertAlmostEqual(np.sum(P), 1.0, delta=1e-6, msg=\"P not normalized\")\n",
    "\n",
    "    def test_compute_P_various_perplexities(self):\n",
    "        \"\"\"Ensure stability across different perplexity values.\"\"\"\n",
    "        for p in [2, 5, 10]:\n",
    "            self.tsne.perplexity = p\n",
    "            P = self.tsne._compute_P(self.X)\n",
    "            self.assertAlmostEqual(np.sum(P), 1.0, delta=1e-6, msg=f\"P not normalized for perplexity={p}\")\n",
    "            self.assertTrue(np.allclose(P, P.T, atol=1e-8), f\"P not symmetric for perplexity={p}\")\n",
    "    \n",
    "    def test_compute_Q(self):\n",
    "        \"\"\"Check if the Q matrix in low-dimensional space is valid and normalized.\"\"\"\n",
    "        # Simple 3-point 2D embedding\n",
    "        Y = np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n",
    "        Q, num = self.tsne._compute_Q(Y)\n",
    "        \n",
    "        # Q should be symmetric\n",
    "        self.assertTrue(np.allclose(Q, Q.T, atol=1e-8), \"Q is not symmetric\")\n",
    "        # Diagonal should be zero\n",
    "        self.assertTrue(np.allclose(np.diag(Q), 0.0), \"Diagonal of Q should be 0\")\n",
    "        # All probabilities should be positive\n",
    "        self.assertTrue(np.all(Q > 0), \"Q contains non-positive values\")\n",
    "        # Q should be normalized\n",
    "        self.assertAlmostEqual(np.sum(Q), 1.0, delta=1e-6, msg=\"Q not normalized\")\n",
    "\n",
    "    def test_compute_Q_single_point(self):\n",
    "        \"\"\"Edge case: single point should yield all zeros.\"\"\"\n",
    "        Y = np.array([[0.0, 0.0]])\n",
    "        Q, num = self.tsne._compute_Q(Y)\n",
    "        self.assertTrue(np.allclose(Q, np.zeros_like(Q)), \"Single point Q should be zeros\")\n",
    "\n",
    "    def test_compute_Q_identical_points(self):\n",
    "        \"\"\"Edge case: identical points should not cause NaN or inf.\"\"\"\n",
    "        Y = np.array([[1.0, 1.0], [1.0, 1.0]])\n",
    "        Q, num = self.tsne._compute_Q(Y)\n",
    "        self.assertFalse(np.any(np.isnan(Q)), \"Q contains NaNs\")\n",
    "        self.assertFalse(np.any(np.isinf(Q)), \"Q contains infinities\")\n",
    "        self.assertAlmostEqual(np.sum(Q), 1.0, delta=1e-6, msg=\"Q not normalized\")\n",
    "\n",
    "    def test_compute_Q_large_values(self):\n",
    "        \"\"\"Edge case: large coordinate magnitudes should not overflow.\"\"\"\n",
    "        Y = np.array([[1000, 1000], [1003, 1004], [1006, 1008]])\n",
    "        Q, num = self.tsne._compute_Q(Y)\n",
    "        self.assertTrue(np.all(Q >= 0), \"Q contains negative values\")\n",
    "        self.assertAlmostEqual(np.sum(Q), 1.0, delta=1e-6, msg=\"Q not normalized\")\n",
    "    \n",
    "    def test_gradient_basic(self):\n",
    "        \"\"\"Check that gradient has correct shape and no NaNs.\"\"\"\n",
    "        X = np.array([[0, 0], [3, 4], [6, 8]])\n",
    "        P = self.tsne._compute_P(X)\n",
    "        Y = np.random.randn(3, 2) * 0.01\n",
    "        Q, num = self.tsne._compute_Q(Y)\n",
    "        grad = self.tsne._gradient(P, Q, Y, num)\n",
    "\n",
    "        # Check shape\n",
    "        self.assertEqual(grad.shape, Y.shape, \"Gradient shape mismatch\")\n",
    "        # Check finite values\n",
    "        self.assertFalse(np.any(np.isnan(grad)), \"Gradient contains NaN values\")\n",
    "        self.assertFalse(np.any(np.isinf(grad)), \"Gradient contains Inf values\")\n",
    "\n",
    "    def test_gradient_zero_when_P_equals_Q(self):\n",
    "        \"\"\"Gradient should be near zero when P == Q.\"\"\"\n",
    "        Y = np.random.randn(3, 2) * 0.01\n",
    "        Q, num = self.tsne._compute_Q(Y)\n",
    "        P = Q.copy()\n",
    "        grad = self.tsne._gradient(P, Q, Y, num)\n",
    "        self.assertTrue(np.allclose(grad, 0, atol=1e-6), \"Gradient should vanish when P=Q\")\n",
    "\n",
    "    def test_gradient_single_point(self):\n",
    "        \"\"\"Edge case: single point should yield zero gradient.\"\"\"\n",
    "        P = np.zeros((1, 1))\n",
    "        Q = np.zeros((1, 1))\n",
    "        num = np.zeros((1, 1))\n",
    "        Y = np.array([[0.0, 0.0]])\n",
    "        grad = self.tsne._gradient(P, Q, Y, num)\n",
    "        self.assertTrue(np.allclose(grad, np.zeros_like(Y)), \"Single point gradient should be zero\")\n",
    "\n",
    "    def test_gradient_identical_points(self):\n",
    "        \"\"\"Edge case: identical points should produce finite gradients.\"\"\"\n",
    "        X = np.array([[1.0, 1.0], [1.0, 1.0]])\n",
    "        P = self.tsne._compute_P(X)\n",
    "        Y = np.array([[0.0, 0.0], [0.0, 0.0]])\n",
    "        Q, num = self.tsne._compute_Q(Y)\n",
    "        grad = self.tsne._gradient(P, Q, Y, num)\n",
    "        self.assertFalse(np.any(np.isnan(grad)), \"Gradient contains NaNs for identical points\")\n",
    "        self.assertFalse(np.any(np.isinf(grad)), \"Gradient contains infs for identical points\")\n",
    "\n",
    "    def test_gradient_large_values(self):\n",
    "        \"\"\"Edge case: large coordinates should not overflow.\"\"\"\n",
    "        X = np.array([[1000, 1000], [1003, 1004], [1006, 1008]])\n",
    "        P = self.tsne._compute_P(X)\n",
    "        Y = np.array([[1000.0, 1000.0], [1003.0, 1004.0], [1006.0, 1008.0]])\n",
    "        Q, num = self.tsne._compute_Q(Y)\n",
    "        grad = self.tsne._gradient(P, Q, Y, num)\n",
    "        self.assertTrue(np.all(np.isfinite(grad)), \"Gradient overflow for large values\")\n",
    "    \n",
    "    def test_fit_transform_shape(self):\n",
    "        \"\"\"Check output shape matches (n_samples, n_components).\"\"\"\n",
    "        X = np.array([[0, 0], [3, 4], [6, 8]])\n",
    "        Y = self.tsne.fit_transform(X)\n",
    "        self.assertEqual(Y.shape, (X.shape[0], self.tsne.n_components), \"Output shape mismatch\")\n",
    "\n",
    "    def test_fit_transform_convergence_trend(self):\n",
    "        \"\"\"Check that KL divergence generally decreases overall.\"\"\"\n",
    "        X = np.random.randn(10, 5)\n",
    "        tsne = CustomTSNE(n_iter=300, perplexity=5)\n",
    "\n",
    "        # Compute initial KL divergence\n",
    "        P = tsne._compute_P(X)\n",
    "        Y = np.random.randn(X.shape[0], tsne.n_components) * 1e-4\n",
    "        Q, _ = tsne._compute_Q(Y)\n",
    "        initial_kl = np.sum(P * np.log((P + 1e-12) / (Q + 1e-12)))\n",
    "\n",
    "        # Train normally\n",
    "        Y_final = tsne.fit_transform(X)\n",
    "        Q_final, _ = tsne._compute_Q(Y_final)\n",
    "        final_kl = np.sum(P * np.log((P + 1e-12) / (Q_final + 1e-12)))\n",
    "\n",
    "        # Assert only overall improvement\n",
    "        self.assertLess(final_kl, initial_kl, \"Final KL divergence did not decrease overall\")\n",
    "\n",
    "    \n",
    "    def test_fit_transform_global_kl_reduction(self):\n",
    "        \"\"\"Ensure the final KL divergence is much lower than initial (global convergence).\"\"\"\n",
    "        X = np.random.randn(20, 5)\n",
    "        tsne = CustomTSNE(n_iter=300, perplexity=5)\n",
    "        \n",
    "        # Compute initial KL divergence (with random small Y)\n",
    "        P = tsne._compute_P(X)\n",
    "        Y_init = np.random.randn(X.shape[0], tsne.n_components) * 1e-4\n",
    "        Q_init, _ = tsne._compute_Q(Y_init)\n",
    "        initial_kl = np.sum(P * np.log((P + 1e-12) / (Q_init + 1e-12)))\n",
    "        \n",
    "        # Run the actual embedding\n",
    "        Y_final = tsne.fit_transform(X)\n",
    "        Q_final, _ = tsne._compute_Q(Y_final)\n",
    "        final_kl = np.sum(P * np.log((P + 1e-12) / (Q_final + 1e-12)))\n",
    "        \n",
    "        # Assert large reduction in divergence\n",
    "        self.assertLess(final_kl, initial_kl * 0.5, \"KL divergence not significantly reduced\")\n",
    "\n",
    "    def test_fit_transform_single_point(self):\n",
    "        \"\"\"Edge case: single data point should yield zero embedding.\"\"\"\n",
    "        X = np.array([[0.0, 0.0]])\n",
    "        Y = self.tsne.fit_transform(X)\n",
    "        self.assertTrue(np.allclose(Y, np.zeros_like(Y), atol=1e-6), \"Single point embedding should be zero\")\n",
    "\n",
    "    def test_fit_transform_identical_points(self):\n",
    "        \"\"\"Edge case: identical points should not diverge.\"\"\"\n",
    "        X = np.array([[1.0, 1.0], [1.0, 1.0]])\n",
    "        Y = self.tsne.fit_transform(X)\n",
    "        self.assertFalse(np.any(np.isnan(Y)), \"Embedding contains NaNs for identical points\")\n",
    "        self.assertFalse(np.any(np.isinf(Y)), \"Embedding contains infs for identical points\")\n",
    "        self.assertLess(np.max(np.abs(Y)), 1.0, \"Identical points embedding diverged\")\n",
    "\n",
    "    def test_fit_transform_stability_large_values(self):\n",
    "        \"\"\"Edge case: large-magnitude input should not overflow.\"\"\"\n",
    "        X = np.array([[1000, 1000], [1003, 1004], [1006, 1008]])\n",
    "        Y = self.tsne.fit_transform(X)\n",
    "        self.assertTrue(np.all(np.isfinite(Y)), \"Embedding overflow for large input values\")\n",
    "\n",
    "    def test_fit_transform_small_vs_large_perplexity(self):\n",
    "        \"\"\"Ensure results differ across very small vs. large perplexity.\"\"\"\n",
    "        X = np.random.randn(20, 4)\n",
    "        tsne_small = CustomTSNE(perplexity=5, n_iter=300)\n",
    "        tsne_large = CustomTSNE(perplexity=50, n_iter=300)\n",
    "        Y_small = tsne_small.fit_transform(X)\n",
    "        Y_large = tsne_large.fit_transform(X)\n",
    "        self.assertFalse(np.allclose(Y_small, Y_large, atol=1e-3), \"Perplexity has no effect on result\")\n",
    "\n",
    "    def test_fit_transform_reproducibility(self):\n",
    "        \"\"\"Ensure deterministic output for same random seed.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        X = np.random.randn(8, 4)\n",
    "        tsne1 = CustomTSNE(n_iter=200)\n",
    "        tsne2 = CustomTSNE(n_iter=200)\n",
    "        Y1 = tsne1.fit_transform(X)\n",
    "        np.random.seed(42)\n",
    "        Y2 = tsne2.fit_transform(X)\n",
    "        self.assertTrue(np.allclose(Y1, Y2, atol=1e-6), \"Results not reproducible with same seed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run all tests in the notebook\n",
    "unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestCustomTSNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24f179351fa008",
   "metadata": {
    "collapsed": false,
    "id": "df24f179351fa008"
   },
   "source": [
    "# Load data\n",
    "Please use the cell below to discuss your dataset choice and why it is appropriate (or not) for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4083f-5267-44d3-89ed-65864f82aa57",
   "metadata": {
    "id": "74c4083f-5267-44d3-89ed-65864f82aa57"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a3b8890e86f9",
   "metadata": {
    "id": "a14a3b8890e86f9"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Normalize data if necessary\n",
    "\n",
    "# Split the data into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49bb42f79a55f",
   "metadata": {
    "collapsed": false,
    "id": "da49bb42f79a55f"
   },
   "source": [
    "# t-SNE demonstration\n",
    "Demonstrate your t-SNE implementation.\n",
    "\n",
    "Add plots and figures. The code below is just to help you get started, and should not be your final submission.\n",
    "\n",
    "Please use the cell below to describe your results and tests.\n",
    "\n",
    "Describe the difference between your implementation and the sklearn implementation. Hint: you can look at the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064afb5-aeea-48d8-b315-921bf4f8238f",
   "metadata": {
    "id": "a064afb5-aeea-48d8-b315-921bf4f8238f"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3628856e1335fd",
   "metadata": {
    "id": "9b3628856e1335fd"
   },
   "outputs": [],
   "source": [
    "# Run your custom t-SNE implementation\n",
    "custom_tsne = CustomTSNE(n_components=2, perplexity=N/10)\n",
    "custom_Y = custom_tsne.fit_transform(X_train)\n",
    "\n",
    "# Run sklearn t-SNE\n",
    "sk_tsne = TSNE(n_components=2, init='random', perplexity=N/10)\n",
    "sk_Y = sk_tsne.fit_transform(X_train)\n",
    "\n",
    "# Visualization of the result\n",
    "plt.figure()\n",
    "plt.scatter(custom_Y[:, 0], custom_Y[:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "plt.scatter(custom_Y[:, 0], custom_Y[:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('MNIST Data Embedded into 2D with Custom t-SNE')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(sk_Y[:, 0], sk_Y[:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('MNIST Data Embedded into 2D with sklearn t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa2fceedc77e92",
   "metadata": {
    "collapsed": false,
    "id": "73fa2fceedc77e92"
   },
   "source": [
    "# t-SNE extension - mapping new samples\n",
    "Demonstrate your t-SNE transformation procedure.\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below t describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34701c-cc3b-439a-b2d0-393449cf5a20",
   "metadata": {
    "id": "7b34701c-cc3b-439a-b2d0-393449cf5a20"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38dc132b23e7b",
   "metadata": {
    "id": "9d38dc132b23e7b"
   },
   "outputs": [],
   "source": [
    "# Transform new data\n",
    "custom_Y_new = custom_tsne.transform(X_train,custom_Y,X_test)\n",
    "\n",
    "# Visualization of the result\n",
    "plt.figure()\n",
    "plt.scatter(custom_Y[:, 0], custom_Y[:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "plt.scatter(custom_Y_new[:, 0], custom_Y_new[:, 1], marker = '*', s=50, linewidths=0.5, edgecolors='k', c=label_test.astype(int), cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('MNIST Data Embedded into 2D with Custom t-SNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c95c7f-d3a9-4e3d-b539-02e020358766",
   "metadata": {
    "id": "18c95c7f-d3a9-4e3d-b539-02e020358766"
   },
   "source": [
    "# Use of generative AI\n",
    "Please use the cell below to describe your use of generative AI in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6",
   "metadata": {
    "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
